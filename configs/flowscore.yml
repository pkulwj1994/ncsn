training:
  batch_size: 8
  n_epochs: 500000
  n_iters: 200001
  ngpu: 1
  snapshot_freq: 5000
  algo: 'dsm'
  n_particles: 1
  anneal_power: 2.0
  lam: 1.0
  
flow_training:
  batch_size: 128
  lr_flow: 0.0005
  weight_decay: 0.00005
  warm_up: 200
  max_grad_norm: 1.

data:
  ## mnist
  # dataset: "MNIST"
  # image_size: 28
  # channels: 1
  # logit_transform: false
  # random_flip: false
  ## celeba
#  dataset: "CELEBA"
#  image_size: 32
#  channels: 3
#  logit_transform: false
#  random_flip: true
#         cifar10
  dataset: "CIFAR10"
  image_size: 32
  channels: 3
  logit_transform: false
  random_flip: true

model:
  sigma_begin: 1
  sigma_end: 0.01
  num_classes: 10
  batch_norm: false
  ## configurations for CelebA, CIFAR10
#  ngf: 128
  ### configurations for MNIST
  ngf: 64
  
flow_model:
  num_channels: 96
  num_blocks: 10
  num_dequant_blocks: 2
  num_components: 32
  use_attn: true
  drop_prob: 0.2

optim:
  weight_decay: 0.000
  optimizer: "Adam"
  lr: 0.001
  beta1: 0.9
  amsgrad: false
